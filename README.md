# Network Security ML Pipeline

A production-ready Machine Learning pipeline for Network Security analysis with MongoDB Atlas integration. This project implements a modular, scalable architecture following ML engineering best practices.

## ğŸ“‹ Prerequisites

- Python 3.8+
- MongoDB Atlas account (or local MongoDB instance)
- Git

## ğŸ› ï¸ Installation

### 1. Clone the Repository

```bash
git clone <your-repo-url>
cd NetworkSecurity
```

### 2. Create Virtual Environment

```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Linux/Mac
python3 -m venv venv
source venv/bin/activate
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

### 4. Install Package in Development Mode

```bash
pip install -e .
```

## âš™ï¸ Configuration

### MongoDB Atlas Setup

1. Create a MongoDB Atlas account at [mongodb.com](https://www.mongodb.com/cloud/atlas)
2. Create a new cluster and database
3. Get your connection string (MongoDB URI)
4. Create a `.env` file in the root directory:

```env
MONGO_DB_URL=your_mongodb_atlas_connection_string_here
```

**Example:**
```env
MONGO_DB_URL=mongodb+srv://username:password@cluster0.xxxxx.mongodb.net/?retryWrites=true&w=majority
```

### Database Configuration

Update the database and collection names in `networksecurity/constant/training_pipeline/__init__.py`:

```python
DATA_INGESTION_DATABASE_NAME: str = "YourDatabaseName"
DATA_INGESTION_COLLECTION_NAME: str = "YourCollectionName"
```

## ğŸ“Š Data Setup

### Upload Data to MongoDB Atlas

1. Place your CSV file in `Network_Data/` directory (e.g., `phisingData.csv`)
2. Run the data upload script:

```bash
python push_data.py
```

This will:
- Read the CSV file
- Convert it to JSON format
- Insert records into MongoDB Atlas

**Note:** Update `DATABASE` and `Collection` variables in `push_data.py` to match your MongoDB setup.

## ğŸ¯ Usage

### Run the Data Ingestion Pipeline

```bash
python main.py
```

This will:
1. Connect to MongoDB Atlas
2. Export data as DataFrame
3. Save to feature store (CSV)
4. Split data into train/test sets (80/20)
5. Save train.csv and test.csv in artifact directory
6. Return artifact with file paths

### Output Structure

```
Artifacts/
â””â”€â”€ MM_DD_YYYY_HH_MM_SS/
    â””â”€â”€ data_ingestion/
        â”œâ”€â”€ feature_store/
        â”‚   â””â”€â”€ phisingData.csv
        â””â”€â”€ ingested/
            â”œâ”€â”€ train.csv
            â””â”€â”€ test.csv
```

## ğŸ“ Project Structure

```
NetworkSecurity/
â”œâ”€â”€ networksecurity/              # Main package
â”‚   â”œâ”€â”€ components/               # Pipeline components
â”‚   â”‚   â””â”€â”€ data_ingestion.py    # Data ingestion logic
â”‚   â”œâ”€â”€ constant/                # Configuration constants
â”‚   â”‚   â””â”€â”€ training_pipeline/
â”‚   â”‚       â””â”€â”€ __init__.py      # Pipeline constants
â”‚   â”œâ”€â”€ entity/                  # Data classes
â”‚   â”‚   â”œâ”€â”€ artifact_entity.py   # Artifact data classes
â”‚   â”‚   â””â”€â”€ config_entity.py     # Configuration entities
â”‚   â”œâ”€â”€ exception/               # Custom exceptions
â”‚   â”‚   â””â”€â”€ exception.py
â”‚   â”œâ”€â”€ logging/                 # Logging configuration
â”‚   â”‚   â””â”€â”€ logger.py
â”‚   â”œâ”€â”€ pipeline/                # Pipeline orchestration
â”‚   â””â”€â”€ utils/                   # Utility functions
â”œâ”€â”€ Network_Data/                 # Data directory
â”‚   â””â”€â”€ phisingData.csv
â”œâ”€â”€ logs/                        # Log files
â”œâ”€â”€ Artifacts/                   # Generated artifacts
â”œâ”€â”€ main.py                      # Entry point
â”œâ”€â”€ push_data.py                 # Data upload script
â”œâ”€â”€ requirements.txt             # Dependencies
â”œâ”€â”€ setup.py                     # Package setup
â””â”€â”€ .env                         # Environment variables (not in repo)
```

## ğŸ› ï¸ Technologies Used

- **Python 3.8+**
- **pandas** - Data manipulation
- **numpy** - Numerical operations
- **pymongo** - MongoDB driver
- **scikit-learn** - Machine learning utilities
- **python-dotenv** - Environment variable management
- **certifi** - SSL certificate verification

## ğŸ“ Configuration Constants

Key constants can be modified in `networksecurity/constant/training_pipeline/__init__.py`:

- `TARGET_COLUMN`: Target variable name
- `DATA_INGESTION_TRAIN_TEST_SPLIT_RATION`: Train/test split ratio (default: 0.2)
- `DATA_INGESTION_DATABASE_NAME`: MongoDB database name
- `DATA_INGESTION_COLLECTION_NAME`: MongoDB collection name

## ğŸ” Current Status

âœ… **Completed:**
- MongoDB Atlas integration
- Data ingestion pipeline
- Feature store management
- Train/test data splitting
- Logging and exception handling
- Artifact management

ğŸš§ **In Progress:**
- Data validation component
- Data transformation component

## ğŸ“ Logging

Logs are automatically generated in the `logs/` directory with timestamp format:
```
logs/MM_DD_YYYY_HH_MM_SS.log/MM_DD_YYYY_HH_MM_SS.log
```

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ‘¤ Author

**Priyanshu Ranjan**
- Email: priyanshujaiz341@gmail.com

**Note:** Make sure to add `.env` to your `.gitignore` file to keep your MongoDB credentials secure!


## Quick Start

1. Clone repo â†’ `git clone <repo-url>`
2. Create venv â†’ `python -m venv venv` â†’ `venv\Scripts\activate`
3. Install deps â†’ `pip install -r requirements.txt` â†’ `pip install -e .`
4. Add `.env` â†’ `MONGO_DB_URL=your_connection_string`
5. Upload data â†’ `python push_data.py`
6. Run pipeline â†’ `python main.py`
